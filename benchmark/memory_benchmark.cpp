#include <base/base.h>
#include <base/tick.h>
#include <glog/logging.h>
#include <chrono>
#include <vector>
#include <iomanip>
#include <iostream>
#include <fstream>
#include <sstream>
#include "model/llama3.h"

// æ·»åŠ CUDAå¤´æ–‡ä»¶ç”¨äºGPUå†…å­˜ç›‘æ§
#include <cuda_runtime.h>
#include <cuda.h>

#ifdef __linux__
#include <unistd.h>
#include <sys/resource.h>
#endif

class MemoryBenchmark {
private:
    const model::LLama3Model& model_;
    size_t baseline_cpu_memory_mb_;
    size_t baseline_gpu_memory_mb_;
    size_t model_gpu_memory_mb_;  // æ¨¡å‹æœ¬èº«å ç”¨çš„GPUå†…å­˜
    
public:
    explicit MemoryBenchmark(const model::LLama3Model& model) : model_(model) {
        // è®°å½•æ¨¡å‹åŠ è½½åçš„åŸºå‡†å†…å­˜ä½¿ç”¨
        baseline_cpu_memory_mb_ = get_memory_usage_mb();
        baseline_gpu_memory_mb_ = get_gpu_memory_usage_mb();
        
        // æ³¨æ„ï¼šè¿™é‡Œæ— æ³•å‡†ç¡®è·å–æ¨¡å‹æœ¬èº«çš„GPUå†…å­˜å ç”¨
        // å› ä¸ºæ¨¡å‹åœ¨æ„é€ å‡½æ•°ä¸­å·²ç»åŠ è½½å®Œæˆ
        // å»ºè®®åœ¨mainå‡½æ•°ä¸­æ¨¡å‹åˆå§‹åŒ–å‰ååˆ†åˆ«æµ‹é‡
        model_gpu_memory_mb_ = 0; // éœ€è¦å¤–éƒ¨ä¼ å…¥
    }
    
    // è®¾ç½®æ¨¡å‹å ç”¨çš„GPUå†…å­˜ï¼ˆåœ¨mainå‡½æ•°ä¸­è°ƒç”¨ï¼‰
    void set_model_gpu_memory(size_t model_memory_mb) {
        model_gpu_memory_mb_ = model_memory_mb;
    }
    
    // è·å–å½“å‰è¿›ç¨‹å†…å­˜ä½¿ç”¨ (Linux)
    size_t get_memory_usage_mb() {
#ifdef __linux__
        std::ifstream status_file("/proc/self/status");
        std::string line;
        while (std::getline(status_file, line)) {
            if (line.substr(0, 6) == "VmRSS:") {
                std::istringstream iss(line);
                std::string label, value, unit;
                iss >> label >> value >> unit;
                return std::stoll(value) / 1024; // è½¬æ¢ä¸ºMB
            }
        }
#endif
        return 0;
    }
    
    // è·å–GPUå†…å­˜ä½¿ç”¨æƒ…å†µ
    size_t get_gpu_memory_usage_mb() {
        size_t free_memory, total_memory;
        cudaError_t result = cudaMemGetInfo(&free_memory, &total_memory);
        if (result != cudaSuccess) {
            std::cerr << "CUDAå†…å­˜ä¿¡æ¯è·å–å¤±è´¥: " << cudaGetErrorString(result) << std::endl;
            return 0;
        }
        return (total_memory - free_memory) / (1024 * 1024); // è½¬æ¢ä¸ºMB
    }
    
    // è·å–GPUæ€»å†…å­˜
    size_t get_gpu_total_memory_mb() {
        size_t free_memory, total_memory;
        cudaError_t result = cudaMemGetInfo(&free_memory, &total_memory);
        if (result != cudaSuccess) {
            std::cerr << "CUDAå†…å­˜ä¿¡æ¯è·å–å¤±è´¥: " << cudaGetErrorString(result) << std::endl;
            return 0;
        }
        return total_memory / (1024 * 1024); // è½¬æ¢ä¸ºMB
    }
    
    // è·å–ç›¸å¯¹äºåŸºå‡†çš„å†…å­˜å˜åŒ–
    size_t get_cpu_memory_change_mb() {
        return get_memory_usage_mb() - baseline_cpu_memory_mb_;
    }
    
    size_t get_gpu_memory_change_mb() {
        return get_gpu_memory_usage_mb() - baseline_gpu_memory_mb_;
    }
    
    // åŸºç¡€å†…å­˜ä½¿ç”¨æµ‹è¯•
    void test_base_memory_usage() {
        std::cout << "\n=== åŸºç¡€å†…å­˜ä½¿ç”¨æµ‹è¯• ===\n";
        
        size_t current_cpu_memory = get_memory_usage_mb();
        size_t current_gpu_memory = get_gpu_memory_usage_mb();
        size_t gpu_total = get_gpu_total_memory_mb();
        
        std::cout << "æ¨¡å‹åŠ è½½åå†…å­˜ä½¿ç”¨:\n";
        std::cout << "  CPUå†…å­˜: " << current_cpu_memory << " MB (åŸºå‡†: " << baseline_cpu_memory_mb_ << " MB)\n";
        std::cout << "  GPUæ€»å†…å­˜: " << current_gpu_memory << " MB / " << gpu_total << " MB (" 
                  << std::fixed << std::setprecision(1) 
                  << (static_cast<double>(current_gpu_memory) / gpu_total * 100) << "%)\n";
        std::cout << "  GPUåŸºå‡†å†…å­˜: " << baseline_gpu_memory_mb_ << " MB\n";
        
        if (model_gpu_memory_mb_ > 0) {
            std::cout << "  ğŸ¯ æ¨¡å‹æœ¬èº«å ç”¨GPUå†…å­˜: " << model_gpu_memory_mb_ << " MB\n";
            std::cout << "  ğŸ“Š å…¶ä»–è¿›ç¨‹å ç”¨GPUå†…å­˜: " << (current_gpu_memory - model_gpu_memory_mb_) << " MB\n";
        } else {
            std::cout << "  âš ï¸  æ— æ³•å‡†ç¡®è®¡ç®—æ¨¡å‹GPUå†…å­˜å ç”¨ï¼ˆå¯èƒ½GPUä¸Šæœ‰å…¶ä»–è¿›ç¨‹ï¼‰\n";
        }
        
        // æ³¨æ„ï¼šç”±äºconfig_æ˜¯ç§æœ‰æˆå‘˜ï¼Œè¿™é‡Œä½¿ç”¨ä¼°ç®—å€¼
        // åŸºäºLLaMA3çš„å…¸å‹é…ç½®è¿›è¡Œä¼°ç®—
        const int estimated_layer_num = 32;
        const int estimated_seq_len = 2048;
        const int estimated_kv_dim = 1024;  // å…¸å‹GQAé…ç½®
        
        size_t kv_cache_size_mb = (estimated_layer_num * estimated_seq_len * estimated_kv_dim * sizeof(float) * 2) / (1024 * 1024);
        std::cout << "KV Cacheä¼°è®¡å¤§å°: " << kv_cache_size_mb << " MB (åŸºäºå…¸å‹é…ç½®)\n";
    }
    
    // åºåˆ—é•¿åº¦å¯¹å†…å­˜å½±å“æµ‹è¯•
    void test_sequence_length_memory() {
        std::cout << "\n=== åºåˆ—é•¿åº¦å†…å­˜å½±å“æµ‹è¯• ===\n";
        
        std::vector<int> seq_lengths = {64, 128, 256, 512, 1024};
        std::string base_prompt = "Memory usage test prompt";
        
        for (int seq_len : seq_lengths) {
            size_t cpu_memory_before = get_memory_usage_mb();
            size_t gpu_memory_before = get_gpu_memory_usage_mb();
            
            // ç”ŸæˆæŒ‡å®šé•¿åº¦çš„åºåˆ—
            generate_sequence(base_prompt, seq_len);
            
            size_t cpu_memory_after = get_memory_usage_mb();
            size_t gpu_memory_after = get_gpu_memory_usage_mb();
            size_t cpu_memory_diff = cpu_memory_after - cpu_memory_before;
            size_t gpu_memory_diff = gpu_memory_after - gpu_memory_before;
            
            std::cout << "åºåˆ—é•¿åº¦ " << seq_len << ":\n";
            std::cout << "  CPUå†…å­˜å˜åŒ–: " << cpu_memory_diff << " MB (ç›¸å¯¹åŸºå‡†: " << get_cpu_memory_change_mb() << " MB)\n";
            std::cout << "  GPUå†…å­˜å˜åŒ–: " << gpu_memory_diff << " MB (ç›¸å¯¹åŸºå‡†: " << get_gpu_memory_change_mb() << " MB)\n";
        }
    }
    
    // KV Cacheå¢é•¿æµ‹è¯•
    void test_kv_cache_growth() {
        std::cout << "\n=== KV Cacheå¢é•¿æµ‹è¯• ===\n";
        
        std::string prompt = "Testing KV cache memory growth";
        auto tokens = model_.encode(prompt);
        
        std::vector<size_t> cpu_memory_changes;
        std::vector<size_t> gpu_memory_changes;
        std::vector<int> positions;
        
        // å¤„ç†promptå¹¶ç”Ÿæˆtokensï¼Œå®šæœŸè®°å½•å†…å­˜ä½¿ç”¨
        auto prompt_embedding = model_.embedding(tokens);
        tensor::Tensor pos_tensor = model_.get_buffer(model::ModelBufferType::kInputPos);
        
        cpu_memory_changes.push_back(get_cpu_memory_change_mb());
        gpu_memory_changes.push_back(get_gpu_memory_change_mb());
        positions.push_back(0);
        
        // å¤„ç†prompté˜¶æ®µ
        for (int32_t pos = 0; pos < static_cast<int32_t>(tokens.size()) - 1; ++pos) {
            pos_tensor.index<int32_t>(0) = pos;
            tensor::Tensor input = model_.fill_input(pos_tensor, prompt_embedding, true);
            int32_t next = -1;
            model_.predict(input, pos_tensor, true, next);
            
            if (pos % 10 == 0) {
                cpu_memory_changes.push_back(get_cpu_memory_change_mb());
                gpu_memory_changes.push_back(get_gpu_memory_change_mb());
                positions.push_back(pos);
            }
        }
        
        // ç”Ÿæˆé˜¶æ®µ 
        int32_t current = tokens.back();
        for (int i = 0; i < 100; ++i) {
            std::vector<int32_t> tokens_vec = {current};
            auto embedding = model_.embedding(tokens_vec);
            pos_tensor.index<int32_t>(0) = static_cast<int32_t>(tokens.size()) + i;
            tensor::Tensor input_tensor = model_.fill_input(pos_tensor, embedding, false);
            
            int32_t next = -1;
            model_.predict(input_tensor, pos_tensor, false, next);
            
            if (i % 10 == 0) {
                cpu_memory_changes.push_back(get_cpu_memory_change_mb());
                gpu_memory_changes.push_back(get_gpu_memory_change_mb());
                positions.push_back(static_cast<int>(tokens.size()) + i);
            }
            
            if (model_.is_sentence_ending(next)) {
                break;
            }
            current = next;
        }
        
        // åˆ†æå†…å­˜å¢é•¿è¶‹åŠ¿
        std::cout << "å†…å­˜å˜åŒ–è¶‹åŠ¿ (ç›¸å¯¹äºæ¨¡å‹åŠ è½½å):\n";
        for (size_t i = 0; i < cpu_memory_changes.size(); ++i) {
            std::cout << "Position " << positions[i] << ":\n";
            std::cout << "  CPUå˜åŒ–: " << cpu_memory_changes[i] << " MB";
            if (i > 0) {
                int cpu_diff = static_cast<int>(cpu_memory_changes[i]) - static_cast<int>(cpu_memory_changes[i-1]);
                std::cout << " (æœ¬æ¬¡: " << cpu_diff << ")";
            }
            std::cout << "\n";
            
            std::cout << "  GPUå˜åŒ–: " << gpu_memory_changes[i] << " MB";
            if (i > 0) {
                int gpu_diff = static_cast<int>(gpu_memory_changes[i]) - static_cast<int>(gpu_memory_changes[i-1]);
                std::cout << " (æœ¬æ¬¡: " << gpu_diff << ")";
            }
            std::cout << "\n";
        }
    }
    
    // å†…å­˜æ³„æ¼æ£€æµ‹
    void test_memory_leak() {
        std::cout << "\n=== å†…å­˜æ³„æ¼æ£€æµ‹ ===\n";
        
        std::cout << "åˆå§‹çŠ¶æ€ (æ¨¡å‹åŠ è½½å):\n";
        std::cout << "  CPUåŸºå‡†: " << baseline_cpu_memory_mb_ << " MB\n";
        std::cout << "  GPUåŸºå‡†: " << baseline_gpu_memory_mb_ << " MB\n";
        
        // æ‰§è¡Œå¤šæ¬¡æ¨ç†å¾ªç¯
        std::string test_prompt = "Memory leak detection test";
        for (int iteration = 0; iteration < 10; ++iteration) {
            for (int i = 0; i < 10; ++i) {
                auto tokens = model_.encode(test_prompt);
                auto embedding = model_.embedding(tokens);
                
                // ç®€å•çš„æ¨ç†è¿‡ç¨‹
                tensor::Tensor pos_tensor = model_.get_buffer(model::ModelBufferType::kInputPos);
                pos_tensor.index<int32_t>(0) = 0;
                tensor::Tensor input = model_.fill_input(pos_tensor, embedding, true);
                int32_t next = -1;
                model_.predict(input, pos_tensor, true, next);
            }
            
            std::cout << "è¿­ä»£ " << (iteration + 1) << ":\n";
            std::cout << "  CPUå˜åŒ–: " << get_cpu_memory_change_mb() << " MB\n";
            std::cout << "  GPUå˜åŒ–: " << get_gpu_memory_change_mb() << " MB\n";
        }
        
        double cpu_memory_growth = static_cast<double>(get_cpu_memory_change_mb());
        double gpu_memory_growth = static_cast<double>(get_gpu_memory_change_mb());
        
        std::cout << "å†…å­˜æ³„æ¼æ£€æµ‹ç»“æœ:\n";
        std::cout << "  CPUå†…å­˜å˜åŒ–: " << cpu_memory_growth << " MB\n";
        std::cout << "  GPUå†…å­˜å˜åŒ–: " << gpu_memory_growth << " MB\n";
        
        if (cpu_memory_growth > 100 || gpu_memory_growth > 100) {
            std::cout << "âš ï¸  å¯èƒ½å­˜åœ¨å†…å­˜æ³„æ¼\n";
        } else if (cpu_memory_growth > 20 || gpu_memory_growth > 20) {
            std::cout << "âš ï¸  å†…å­˜å¢é•¿è¾ƒå¤šï¼Œéœ€è¦å…³æ³¨\n";
        } else {
            std::cout << "âœ… å†…å­˜ä½¿ç”¨æ­£å¸¸\n";
        }
    }

private:
    void generate_sequence(const std::string& prompt, int target_length) {
        auto tokens = model_.encode(prompt);
        auto prompt_embedding = model_.embedding(tokens);
        tensor::Tensor pos_tensor = model_.get_buffer(model::ModelBufferType::kInputPos);
        
        // å¤„ç†prompt
        for (int32_t pos = 0; pos < static_cast<int32_t>(tokens.size()) - 1; ++pos) {
            pos_tensor.index<int32_t>(0) = pos;
            tensor::Tensor input = model_.fill_input(pos_tensor, prompt_embedding, true);
            int32_t next = -1;
            model_.predict(input, pos_tensor, true, next);
        }
        
        // ç”Ÿæˆtokensç›´åˆ°è¾¾åˆ°ç›®æ ‡é•¿åº¦
        int32_t current = tokens.back();
        int current_length = static_cast<int>(tokens.size());
        
        while (current_length < target_length) {
            std::vector<int32_t> tokens_vec = {current};
            auto embedding = model_.embedding(tokens_vec);
            pos_tensor.index<int32_t>(0) = current_length - 1;
            tensor::Tensor input_tensor = model_.fill_input(pos_tensor, embedding, false);
            
            int32_t next = -1;
            model_.predict(input_tensor, pos_tensor, false, next);
            
            if (model_.is_sentence_ending(next)) {
                break;
            }
            
            current = next;
            current_length++;
        }
    }

};

int main(int argc, char* argv[]) {
    if (argc != 3 && argc != 4) {
        LOG(INFO) << "Usage: ./memory_benchmark checkpoint_path tokenizer_path [--quant]";
        LOG(INFO) << "  --quant: ä½¿ç”¨é‡åŒ–æ¨¡å‹ (å¯é€‰ï¼Œé»˜è®¤ä¸ä½¿ç”¨é‡åŒ–)";
        return -1;
    }
    
    const char* checkpoint_path = argv[1];
    const char* tokenizer_path = argv[2];
    
    // æ£€æŸ¥æ˜¯å¦ä½¿ç”¨é‡åŒ–æ¨¡å‹
    bool use_quantization = false;
    if (argc == 4 && std::string(argv[3]) == "--quant") {
        use_quantization = true;
        std::cout << "ä½¿ç”¨é‡åŒ–æ¨¡å‹æ¨¡å¼\n";
    } else {
        std::cout << "ä½¿ç”¨æ ‡å‡†æ¨¡å‹æ¨¡å¼\n";
    }
    
    // è·å–æ¨¡å‹åˆå§‹åŒ–å‰çš„GPUå†…å­˜ä½¿ç”¨
    size_t gpu_memory_before = 0;
    size_t free_memory, total_memory;
    cudaError_t result = cudaMemGetInfo(&free_memory, &total_memory);
    if (result == cudaSuccess) {
        gpu_memory_before = (total_memory - free_memory) / (1024 * 1024);
        std::cout << "æ¨¡å‹åˆå§‹åŒ–å‰GPUå†…å­˜ä½¿ç”¨: " << gpu_memory_before << " MB\n";
    }
    
    // åˆå§‹åŒ–æ¨¡å‹
    model::LLama3Model model(base::TokenizerType::kEncodeBpe , tokenizer_path,
                             checkpoint_path, use_quantization);
    
    auto init_status = model.init(base::DeviceType::kDeviceCUDA);
    if (!init_status) {
        LOG(FATAL) << "æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: " << init_status.get_err_code();
        return -1;
    }
    
    // è·å–æ¨¡å‹åˆå§‹åŒ–åçš„GPUå†…å­˜ä½¿ç”¨
    size_t gpu_memory_after = 0;
    int64_t model_gpu_memory = 0; // å£°æ˜åœ¨å¤–éƒ¨ä½œç”¨åŸŸ
    result = cudaMemGetInfo(&free_memory, &total_memory);
    if (result == cudaSuccess) {
        gpu_memory_after = (total_memory - free_memory) / (1024 * 1024);
        std::cout << "æ¨¡å‹åˆå§‹åŒ–åGPUå†…å­˜ä½¿ç”¨: " << gpu_memory_after << " MB\n";
        // ä½¿ç”¨æœ‰ç¬¦å·æ•´æ•°é¿å…ä¸‹æº¢
        model_gpu_memory = static_cast<int64_t>(gpu_memory_after) - static_cast<int64_t>(gpu_memory_before);
        std::cout << "æ¨¡å‹å ç”¨GPUå†…å­˜: " << model_gpu_memory << " MB\n";
    }
    
    MemoryBenchmark benchmark(model);
    benchmark.set_model_gpu_memory(static_cast<size_t>(std::max<int64_t>(0, model_gpu_memory))); // ä½¿ç”¨è®¡ç®—å‡ºçš„æ¨¡å‹GPUå†…å­˜
    
    std::cout << "=== LLaMA3 å†…å­˜ä½¿ç”¨æµ‹è¯• ===\n";
    std::cout << "æ¨¡å‹ç±»å‹: " << (use_quantization ? "é‡åŒ–æ¨¡å‹" : "æ ‡å‡†æ¨¡å‹") << "\n";
    
    // 1. åŸºç¡€å†…å­˜ä½¿ç”¨
    benchmark.test_base_memory_usage();
    
    // 2. åºåˆ—é•¿åº¦å¯¹å†…å­˜çš„å½±å“
    benchmark.test_sequence_length_memory();
    
    // 3. KV Cacheå¢é•¿æµ‹è¯•  
    benchmark.test_kv_cache_growth();
    
    // 4.å†…å­˜æ³„æ¼æ£€æµ‹
    benchmark.test_memory_leak();
    
    std::cout << "\nå†…å­˜æµ‹è¯•å®Œæˆï¼\n";
    return 0;
} 